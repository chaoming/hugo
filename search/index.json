[{"content":"Why Static Website Builder I had a blog that was using WordPress and it was hosted on an AWS EC2 + CloudFront CDN. It was fast and not too hard to use, except that sometimes it might break down because of a WordPress update or issues with the MySQL database. Since I built the fireact.dev website with Jekyll, I have been thinking to move the old blog site to a static solution because:\nNo need for a database, which means less maintenance headache Free hosting options like Github and Firebase Fast, super fast! That is good for UX and SEO The Solution I did some research and decided not to use Jekyll because I found Hugo is gaining popularity and it has a lot of free themes too. I picked a theme called Stack, which looks great in my opinion and is well maintained by its creator.\nCreating a Hugo site is quite simple following its step-by-step instruction document. The themes are installed as git submodules which makes them relatively simple to install.\nHugo also has a deployment config for Github pages so you can deploy your website by pushing commits to a branch.\nWriting Posts with Markdown The most common way to write content for static websites is to create markdown files. Markdown is widely used in writing documentation for tech projects. You don’t need to worry about HTML tags, just write with some formatting syntax like “##” means tag in HTML.\nIf you don’t want to write with markdown syntax, Notion is a great tool. It has a very user-friendly interface for writing content. Once you finish an article, select the whole article and copy it, and you can paste content to a markdown file.\nMy Thoughts There are many open-source projects are using static website builders for building websites for the projects and are hosting the websites on Github.\nI think it’s totally possible to use the same solution for early-stage startups to build their websites and host them on Firebase. For SaaS startup, my thinking is that a static website for the content, and fireact.dev for the web application. All these can be hosted on Firebase with minimum costs.\nAlthough Notion is a great tool for writing and converting content to markdown format, I think there is a need for some kind of CMS application that can do a similar job and automate the process to update and deploy static websites. Or maybe just automate the deployment process of Notion articles to static websites, because Notion is already doing a really good job as a writing tool. So that people who don’t know much about Git can also easily update their static websites.\n","date":"2022-06-25T00:00:00Z","image":"https://chaoming.li/blog/build-a-static-blog-with-hugo/hugo_hu7bbb3f3ebdb843801f79bded2b036297_73881_120x120_fill_box_smart1_3.png","permalink":"https://chaoming.li/blog/build-a-static-blog-with-hugo/","title":"Build a Static Blog with Hugo"},{"content":"If you want to securely automate deployments of your App Engine application to multiple environments such as staging and production with a simple Git commit or a pull request, this article will outline the steps for you.\nWhy You Should Do This Good reasons to automate the deployment process are:\nReduce the impact of human error on a deployment Improve security by not including any credentials or sensitive configuration settings in the source code Restrict permission to deploy to certain environments (e.g. production) with a process Streamline the deployment process and gain the benefits of automation (e.g. testing) Assumptions I am not going to discuss how to set up Git repositories and GitHub accounts. There are plenty of tutorials online for those topics.\nIn this article, I’ll only assume that you have an App Engine application. I have used this process for Golang and PHP, but it will work for Python and other languages.\nStep 1: Enable Branch Restrictions In your GitHub repository, you need to have some branches, at least a staging branch and a master branch. The workflow is that a developer can commit to the staging branch. It will automatically deploy the application to a staging App Engine environment.\nAfter the application is tested in the staging environment:\nThe developer will create a pull request The administrator will merge it with the master branch The automation will deploy the application to the production environment This workflow is a bit oversimplified. But the point is: Only someone with permission can deploy the application to the production by committing to the master branch. This can be achieved with branch restrictions.\nStep 2: Move All Environment Related Configurations to app.yaml It’s very common for an application to have environment variables for configuration and secrets such as database credentials, domains, GCP key files, etc. Some of these details are security-critical so they should never be included in the source code. If you have embedded them into part of the source code, it’s time to remove them and put them into the app.yaml file that is kept outside of the repository and only is used during deployment.\nIn your app.yaml file, you can have these variables as environment variables in the following syntax:\n1 2 3 4 env_variables: DOMAIN: \u0026#39;staging.mydomain.com\u0026#39; DEBUG_MODE: true MYSQL_DSN: \u0026#39;blah blah\u0026#39; The App Engine platform will take these values from app.yaml and make them available to your application, which can read the environment variables to ensure your application has the correct settings.\nDepending on which language your application is written in, there are different ways to read the environment variables. Here is the link to Python and other languages (click on the language links at the top of the content to switch to your preferred language): https://cloud.google.com/appengine/docs/standard/python/config/appref\nIn PHP, use:\n1 getenv(\u0026#39;VAR NAME\u0026#39;) I don’t think $_ENV works.\nIf you need app.yaml for your local environment, just make sure the file is in .gitignore so it won’t be included in the repository.\nOnce you have put the environment variables into the staging app.yaml file, upload it to a Google Cloud Storage bucket and make sure it’s not available to the public (by default it isn’t). Do the same for the production app.yaml but use a different bucket.\nStep 3: Create Your Cloud Build for Staging In GCP, Cloud Build can trigger a deployment from Git commits to a certain branch. The web UI is very simple to follow to create the trigger. The trick is to have a variable called _BUCKET which is the bucket name that contains your app.yaml\nAnd, you need a cloudbuild.yaml file in your Git repository to control the deployment process. Below is a simple cloudbuild.yaml file that does two things:\nCopy the app.yaml file from the bucket Deploy the application 1 2 3 4 5 steps: - name: gcr.io/cloud-builders/gsutil args: [\u0026#39;cp\u0026#39;, \u0026#39;gs://$_BUCKET/app.yaml\u0026#39;, \u0026#39;html/app.yaml\u0026#39;] - name: \u0026#39;gcr.io/cloud-builders/gcloud\u0026#39; args: [\u0026#39;app\u0026#39;, \u0026#39;deploy\u0026#39;, \u0026#39;html/\u0026#39;] You can see that I use $_BUCKET in the cloudbuild.yaml file as the bucket name of the app.yaml file. This means when I configure the production Cloud Build trigger, the same cloudbuild.yaml will work without any modification.\nNow, commit to your staging branch, and you will see Cloud Build is triggered and starts the deployment process automatically.\nStep 4: Grant Permissions to Cloud Build The deployment is likely to fail because Cloud Build doesn’t have the permissions to access Cloud Storage, App Engine, etc.\nGo to IAM web UI in your GCP project, and change the permission settings of the Cloud Build service account to grant it App Engine Admin and other relevant permissions for completing the deployment. Now, retry the deployment in Cloud Build, you should see the successfully deployed message in the logs.\nStep 5: Repeat Step 3 and Step 4 for Production Just repeat the above two steps in the production project. Make sure the production app.yaml is in the Cloud Storage bucket for production, and the Cloud Build trigger _BUCKET variable is pointing to the production bucket as well.\nBecause only the system administrator is able to commit to production, as a developer, you should create a Pull Request in the GitHub repository to merge your changes from the staging branch into the master branch. Once the Pull Request is approved and merged, it will trigger the Cloud Build deployment process in the production project.\nConclusion:\nThis approach not only streamlines the deployment process, but it also makes it more secure and reliable.\nIf you have any feedback or questions, please feel free to comment below.\n","date":"2019-07-16T00:00:00Z","image":"https://chaoming.li/blog/automate-deployments-to-multiple-app-engine-environments-with-cloud-build-and-github/1_UzYiagjfQwOZTlIwmCqKOQ_hub44496e5fe12534548beed0d5e7745a9_48424_120x120_fill_box_smart1_3.png","permalink":"https://chaoming.li/blog/automate-deployments-to-multiple-app-engine-environments-with-cloud-build-and-github/","title":"Automate Deployments to Multiple App Engine Environments with Cloud Build and GitHub"},{"content":"I recently installed custom SSL certificates in Google Cloud Platform (GCP) for both App Engine and load balancer (used by CDN and Computer Engine). Here is the step-by-step guide on how it’s done. I hope this will help you to go through the process.\nGenerating a certificate signing request To get a certificate, firstly, you need to generate a certificate signing request to your SSL certificate issuer so that they can provide you with a valid certificate. Here is the command using OpenSSL to do it.\n1 openssl req -new -newkey rsa:2048 -nodes -keyout server.key -out server.csr Input the relevant information and put your domain name in Common Name, it will generate two files: server.key which is your private key, and server.csr which is the certificate signing request.\nGet your SSL certificate Provide your certificate issuer with your CSR file, they will generate your SSL certificate. Usually, this process only takes a few minutes. However, the process varies depending on which issuer you are going to use.\nThe format of the certificate can also be different depending on the issuer. I got the X.509 certificate in a crt file in this case.\nConvert your private key to PEM format If you only need to put your certificate in the GCP load balancer, you can skip this step as it doesn’t need a PEM format certificate. This is for App Engine. I don’t get why Google can’t just let me manage all my certificates in one place and in one format. This is actually a bit annoying.\n1 openssl rsa -in server.key -out server.key.pem Upload certificate and PEM key to App Engine In the App Engine settings interface, click on the SSL certificates tab and you can upload a certificate.\nThat will bring up this interface where you can copy and paste your certificate and private key.\nGive your certificate a name. I usually put the year it is issued in it so that when I need to renew it next year, I will have a good idea of which certificate is for which year.\nCopy and paste your X.509 certificate file content and the PEM format private key content in the two boxes. Click “Upload” to save the certificate and key. It will ask you to enable SSL for the custom domains you have in App Engine. Just pick the relevant domains of the certificate.\nThis change will take about 10 minutes or more to become effective. You can verify that by checking your domain’s SSL certificate with any browser. It’s better to check it with all major browsers to make sure they like your certificate.\nUpload certificate to the GCP load balancer To upload a certificate to the GCP load balancer, you need to go to Network services then edit your load balancer. Then click on Frontend configuration and you will be able to edit the port 443 configuration settings where you can manage your SSL certificates.\nAfter clicking on “Create a new certificate” in the Certificate dropdown list, you will see the interface to copy and paste in your certificate, certificate chain (root certificate), and the private key.\nOnce the certificate is created in the system, you can assign it to the load balancer and GCP will ask if you want to delete your unassigned certificate. My suggestion is that only delete it after you can confirm the new certificate is working without any issue.\nSimilar to App Engine, the new certificate will only take effect after about 10 minutes or so. Verify it with all major browsers to make sure they like it.\nIn my experience missing the certificate chain can cause issues with some browsers but Chrome might not mind it.\nConclusion Enabling SSL certificates in GCP load balancer and App Engine is fairly simple if you know about the process. Once you know how to navigate through the GCP UI to do it, it’s fairly straightforward.\nAgain, I think GCP should have a centralised place to manage all the certificates so you can just pick them up in the different products. That will save some effort.\nLeave a comment if you have questions on install SSL certificate on GCP.\n","date":"2019-05-24T00:00:00Z","image":"https://chaoming.li/blog/how-to-install-custom-ssl-certificate-on-google-cloud-platform/1_NG0OmIlxq8M686LHMQHJvg_hu18448c6d191edd9c683761e11ae3b1b2_16669_120x120_fill_q75_box_smart1.jpeg","permalink":"https://chaoming.li/blog/how-to-install-custom-ssl-certificate-on-google-cloud-platform/","title":"How to Install Custom SSL Certificate on Google Cloud Platform"},{"content":"App Engine is great but if you are using Docker, you can only use Flexible Environment which is not what I want as I would love to continue to use Standard Environment. Here is a comparison of the two environments.\nMy goal is to use Docker for development only. When the app is deployed, it will still be deployed to App Engine Standard Environment as usual. After some researches and trial, I found a way to do so.\nCreate App Engine Development Docker After I asked on Stackoverflow, and thanks to dhauptman giving me the direction, I was able to create the Dockerfile that can spin up a docker container with Google Cloud SDK and run Go App Engine development environment. Here is the Dockerfile:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 FROM golang # Install the Google Cloud SDK. RUN echo \u0026#34;deb http://packages.cloud.google.com/apt cloud-sdk-jessie main\u0026#34; | tee /etc/apt/sources.list.d/google-cloud-sdk.list RUN curl https://packages.cloud.google.com/apt/doc/apt-key.gpg | apt-key add - RUN apt-get update \u0026amp;\u0026amp; apt-get install google-cloud-sdk google-cloud-sdk-app-engine-python\\ google-cloud-sdk-app-engine-python-extras\\ google-cloud-sdk-app-engine-java\\ google-cloud-sdk-app-engine-go\\ google-cloud-sdk-datalab\\ google-cloud-sdk-datastore-emulator\\ google-cloud-sdk-pubsub-emulator\\ google-cloud-sdk-cbt\\ google-cloud-sdk-cloud-build-local\\ google-cloud-sdk-bigtable-emulator\\ kubectl -y # Install go packages your app needs RUN go get github.com/go-sql-driver/mysql RUN go get google.golang.org/appengine RUN go get google.golang.org/appengine/log RUN go get google.golang.org/appengine/memcache RUN go get google.golang.org/appengine/taskqueue COPY ./go_app /go/src/app EXPOSE 8080 EXPOSE 8000 CMD [\u0026#34;dev_appserver.py\u0026#34;, \u0026#34;--host=0.0.0.0\u0026#34;, \u0026#34;--admin_host=0.0.0.0\u0026#34;, \u0026#34;--storage_path=~/appengine_storage\u0026#34;, \u0026#34;--blobstore_path=~/appengine_blobstore\u0026#34;, \u0026#34;--datastore_path=~/appengine_datastore\u0026#34;, \u0026#34;--log_level=debug\u0026#34;, \u0026#34;--enable_host_checking=false\u0026#34;, \u0026#34;/go/src/app/app.yaml\u0026#34;] I assume you have basic Docker knowledge so I am not going to explain the Dockerfile step by step here. One thing that cost me some time was the CMD. You must have “–host=0.0.0.0” and “–admin_host=0.0.0.0” in your CMD otherwise you will not be able to access your app engine development instances from your local environment.\nTo save the trouble of typing the docker run parameters. I also created a docker-compose.yaml file. But it’s not really necessary if you have only one Docker container to run.\n1 2 3 4 5 6 7 8 9 10 version: \u0026#39;3\u0026#39; services: go-app-engine: build: . volumes: - ./go_app /go/src/app ports: - 8080:8080 - 8000:8000 Now just need to use the following command to get the Go App Engine development environment up and running.\ndocker-compose up\nDeploy to App Engine Standard Environment I don’t want to deploy Docker containers to App Engine because that allows using Flexible Environment only. I will need to find a different way to deploy to Standard Environment. Google Cloud Build is perfect for the deployment task. It can deploy code from Github repository triggered by Git push.\nThe solution is fairly straightforward. First, push the Go source code to Github. Git push will trigger Cloud Build to run cloudbuild.yaml and go through the steps to get the Go packages and to run the gcloud deploy command.\nTo do this, make sure your App Engine admin API is enabled and the App Engine is created in your project. You will also need to grant App Engine deployer and server admin permissions to the cloud build service account as below screenshot.\nHere is the cloudbuild.yaml file. Similar to the Dockerfile, it specifics the steps to install the Go packages and then run the gcloud deploy command.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 steps: - name: \u0026#39;gcr.io/cloud-builders/go\u0026#39; args: [\u0026#39;get\u0026#39;, \u0026#39;github.com/go-sql-driver/mysql\u0026#39;] env: [\u0026#39;GOPATH=go\u0026#39;] - name: \u0026#39;gcr.io/cloud-builders/go\u0026#39; args: [\u0026#39;get\u0026#39;, \u0026#39;google.golang.org/appengine\u0026#39;] env: [\u0026#39;GOPATH=go\u0026#39;] - name: \u0026#39;gcr.io/cloud-builders/go\u0026#39; args: [\u0026#39;get\u0026#39;, \u0026#39;google.golang.org/appengine/log\u0026#39;] env: [\u0026#39;GOPATH=go\u0026#39;] - name: \u0026#39;gcr.io/cloud-builders/go\u0026#39; args: [\u0026#39;get\u0026#39;, \u0026#39;google.golang.org/appengine/memcache\u0026#39;] env: [\u0026#39;GOPATH=go\u0026#39;] - name: \u0026#39;gcr.io/cloud-builders/go\u0026#39; args: [\u0026#39;get\u0026#39;, \u0026#39;google.golang.org/appengine/taskqueue\u0026#39;] env: [\u0026#39;GOPATH=go\u0026#39;] - name: \u0026#39;gcr.io/cloud-builders/gcloud\u0026#39; args: [\u0026#39;app\u0026#39;, \u0026#39;deploy\u0026#39;, \u0026#39;go_app/\u0026#39;] env: [\u0026#39;GOPATH=go\u0026#39;] artifacts: objects: location: \u0026#39;gs://bucket-nam/\u0026#39; paths: [\u0026#39;go_app\u0026#39;] Now you just need to create the Cloud Build trigger to be triggered by Git push and run the cloudbuild.yaml to package the Go source code and deploy it to App Engine Standard Environment.\nConclusion This approach is great when you need to replicate the development environment without much overhead. Including the Dockerfile in the Git repository will allow new developers to set up a development environment very fast as long as Git and Docker are installed.\nDeployment is fully automatic when the code is pushed to the Github repository. I am not going to talk about how to manage and review code before deployment but you will need to have quality control on the code being pushed into the Github repository anyway.\nLet me know what you think or if you have any questions.\n","date":"2018-09-17T00:00:00Z","image":"https://chaoming.li/blog/how-to-use-docker-for-go-app-engine-development-and-deploy-to-standard-environment/_hua7466efbba06b7d15b358bb2f2b1c17f_32786_6ba3a8547127df54997b67fba017155d.png","permalink":"https://chaoming.li/blog/how-to-use-docker-for-go-app-engine-development-and-deploy-to-standard-environment/","title":"How to Use Docker for Go App Engine Development and Deploy to Standard Environment"},{"content":"BigQuery date partitioned tables can limit the data scan by partitions to help keep the query cost low and improve query performance. However, Google’s documents do not give much clue about how to use partitioned tables to create views that support partition queries. I did some research and found the way to do it, even with views that are created from joining partitioned tables.\nTo expose the partitioning pseudo-column, create a query similar to this one:\n1 2 SELECT *, EXTRACT(DATE FROM _PARTITIONTIME) AS date FROM partitioned-table; If you save the query as a view, you can limit the query partitions by using the date column in your WHERE clause.\nThe above example is probably too simple for any actual query. We often create views because we have complex queries that join multiple tables. In that case, to make the views support partitions, it is just as simple as creating multiple date columns as the above example and making sure your query of the view contains a WHERE clause that limits the search of these date columns. Here is an example of joining two partition tables:\n1 2 3 4 5 6 7 8 SELECT * FROM ( (SELECT *, EXTRACT(DATE FROM _PARTITIONTIME) AS date1 FROM partitioned-table1) t1 LEFT JOIN (SELECT *, EXTRACT(DATE FROM _PARTITIONTIME) AS date2 FROM partitioned-table1) t2 ON t1.key = t2.key ); When you query this view, and you want to limit the query to the data of 2018-03-20, you can do this:\n1 2 SELECT * FROM partitioned-view WHERE date1 = \u0026#39;2018-03-20\u0026#39; AND date2 = \u0026#39;2018-03-20\u0026#39;; I wish I could combine the different date columns as one, so I tried to join the date columns as keys in the ON statement, but that doesn’t help. It seems you always have to have one date column for each partitioned table in the query, that’s a little bit annoying but it will help lower the costs.\n","date":"2018-03-30T00:00:00Z","image":"https://chaoming.li/blog/joining-partitioned-tables-to-create-views-in-bigquery/bigquery-500x250_hu38b1e2727a49b2316807e11208821041_104204_120x120_fill_box_smart1_3.png","permalink":"https://chaoming.li/blog/joining-partitioned-tables-to-create-views-in-bigquery/","title":"Joining Partitioned Tables to Create Views in BigQuery"},{"content":"I gave a presentation in MeasureCamp Melbourne, and the topic was “Build Your Own Web Analytics Platform in 25 Minutes”. In MeasureCamp, each session has 25 minutes of presentation time, that’s where the 25 minutes on the topic from. What inspired me to do this talk was a conversation in Measure Slack about sending Google Analytics data into your own database. And I think it is an interesting thing to do to build a web analytics platform from scratch without spending a lot of time and money.\nBuilding your own web analytics platform can be much simpler than you would think. In this presentation, there is pretty much no coding knowledge required. If you are familiar with AWS S3 and CloudFront, it is very simple. If not, don’t worry, AWS has a really good interface so you just need to click through a series of buttons. Below is the high-level architecture of the platform. Option #1 was something I built in my previous job for production support. It was not based on any cloud so it was able to store personally identifiable information (PII) and the production support team used it to help to recreate scenarios for debugging. Option #2 is the solution in this presentation, which uses AWS cloud so all the infrastructure can be set up in minutes. You will need to have an AWS account to do it. You can sign up for a free account here.\nThe whole process includes 8 steps. I will go through all the 8 steps here.\nStep 1: Create an S3 bucket to store log files First of all, you will need an S3 bucket to store all the log files. AWS S3 is a storage service, where you can store a lot of files in the cloud with pretty low costs. S3 buckets are like folders, you can store a group of files within.\nLogin to your AWS account, click on S3 in the product list menu under the Storage section and click on Create Bucket button. Give a name to the bucket, something like “my-web-analytics-logs”, and click Next all the way through. You have your S3 bucket created now.\nStep 2: Create an S3 bucket to store the tracking pixel image Most web analytics solutions are using an invisible pixel image to track data. They attach the information to be tracked in the pixel URL as query parameters. So we need an S3 bucket to host this pixel file. Just repeat the process of step 1, but name the bucket “my-web-analytics-pixel”. Upload the pixel file to the bucket. You can grab the pixel file from https://s3.amazonaws.com/cli-tracking-demo-pixel/pixel.gif\nYou will need to make the pixel file public so it can be accessed from the Internet. To do so, go to the pixel bucket and tick the checkbox next to the file name, click More and select Make Public.\nStep 3: Create a CloudFront distribution and enable logging Now, we have our pixel file in the cloud, and we are ready to store the log files. CloudFront is the CDN AWS offers. It is fast and cheap to run. The reason we want to use CloudFront here is to be able to serve the pixel file very fast, so it doesn’t impact the page performance much, and log the data at the same time. Go to CloudFront, click on Create Distribution button, and click on Get Started under the Web section to go to the form to create a distribution.\nWhen you create the CloudFront distribution, you will need to enable logging and point to log files to the S3 bucket we created in step 1.\nYou will also need to point the origin to the S3 bucket containing the pixel file so your CloudFront distribution can serve the pixel file. You can leave all the other options as default.\nAfter these 3 steps are completed, you are ready to take in whatever data and store it. We should start to work on the part to generate data.\nStep 4: Create a visitor ID via GTM To track the same visitors across a period, you need a cookie to store a unique random visitor ID. In Google Tag Manager (GTM), go to Variables, and create a custom Javascript variable and name it “Visitor ID”. Copy and paste the code below to the variable. This piece of code will check if the visitor already has a visitor ID cookie “v_id”. If yes, it will read it, refresh the cookie for another 2 years, and return the visitor ID value. Otherwise, it will create a random ID and save it to “v_id” cookie for 2 years, and return the newly created visitor ID value. We will use this value in the next step.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 function(){ var readCookie = function(key){ var result; return (result = new RegExp(\u0026#39;(?:^|; )\u0026#39; + encodeURIComponent(key) + \u0026#39;=([^;]*)\u0026#39;).exec(document.cookie)) ? (result[1]) : null; } var writeCookie = function(name, value, domain, expire){ document.cookie = name+\u0026#34;=\u0026#34;+value+\u0026#34;;domain=.\u0026#34;+domain+\u0026#34;;path=/;expires=\u0026#34;+expire; } var visitorId = readCookie(\u0026#39;v_id\u0026#39;); var date = new Date(); if(visitorId === null){ visitorId = date.getTime().toString(16) + (Math.floor(Math.random() * (999999 - 100000) + 100000)).toString(16); } date.setTime(date.getTime() + (2 * 365 * 24 * 60 * 60 * 1000)); writeCookie(\u0026#39;v_id\u0026#39;, visitorId, location.hostname, new Date(date).toUTCString()); return visitorId; } Step 5: Create an image tag in GTM and attach data in the URL query parameters We are going to use the pixel file in this step. Create an image tag in GTM. In the URL field, put in “https://your-cloud-front-distribution.cloudfront.net/pixel.gif?vid={{Visitor ID}}\u0026amp;pageUrl={{Page URL}}”, set the trigger as All Pages. {{Visitor ID}} is the variable created in the previous step, and I attached the page URL in the query parameters in this example but you can attach whatever variables in your GTM as key-value pairs in the same way.\nStep 6: Publish GTM By now, all the data collection and storage is set up. Once you publish GTM to your websites, data will start to flow in soon. The way it is set up in this demo doesn’t support real-time data streaming, so you will need to wait a few minutes to have your first log file deposited in the S3 bucket.\nStep 7: Create an Athena table schema After the data files are in your S3 bucket, you can start to query the data. AWS Athena is probably the fastest way to do so if you like to use SQL. I prefer to create a table manually in Athena than using the crawler feature because I found the crawler is not always working properly in my experience.\nTo manually create a table in Athena, you will need to point the table to your log file s3 bucket and define all the fields in the table. You can find the CloudFront log file format here. There are 25 fields, so take your time to go through them one by one. In my demo, I only created the first 12 fields because that’s good enough for the demo, and the 12th field is the one stores the query parameter data. Here is a screenshot of the fields in my table.\nStep 8: Check your data You can use “select * from database_name.table_name” to inspect all the data you have in the log files. Spend some time learning your data, you have the IP address, the user agent string, and the data you send through from the pixel.\nHere is the SQL to count visitors by page URLs and sort the results in descending order. The example uses the url_extract_parameter function to get the value out of the query parameters by name. This function only works for full URLs, so I have to concatenate some fields to recreate the full URLs to allow the function to work properly,\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 SELECT COUNT(DISTINCT VisitorID) AS Visitors, PageUrl FROM ( SELECT url_extract_parameter(RequestURL, \u0026#39;vid\u0026#39;) AS VisitorID, url_decode(url_extract_parameter(RequestURL, \u0026#39;url\u0026#39;)) AS PageUrl FROM ( SELECT \u0026#39;https://\u0026#39; || \u0026#34;cs-host\u0026#34; || \u0026#34;cs-uri-stem\u0026#34; || \u0026#39;?\u0026#39; || \u0026#34;cs-uri-query\u0026#34; AS RequestURL FROM \u0026#34;webanalytics\u0026#34;.\u0026#34;logs\u0026#34; ) ) GROUP BY PageUrl ORDER BY Visitors DESC Conclusion By now, you have a very simple web analytics platform that can take in whatever data you want to track as long as you have it in your GTM variables. This is just a super-simplified demo of how web analytics platforms work in principle. There are a lot of missing parts such as cross-domain tracking, data enrichment, data visualisation, etc. However, it is also very flexible because it is not limited by any platform. You have the freedom to design what to track.\nI hope this article helps you understand how web analytics platform collects data at a high level, and feel free to extend it to other usages like tracking non-web data.\n","date":"2018-02-20T00:00:00Z","image":"https://chaoming.li/blog/build-your-own-web-analytics-platform-in-25-minutes/screenshot-2018-02-24-19-46-41_orig-768x427_hub4d3af91cee52a79943037e924e82338_245585_120x120_fill_box_smart1_3.png","permalink":"https://chaoming.li/blog/build-your-own-web-analytics-platform-in-25-minutes/","title":"Build Your Own Web Analytics Platform in 25 Minutes"},{"content":"Back in mid-2015, I created my first and only Python script so far. The script was a voice assistant. I thought I lost the source code, but it’s been in my Dropbox all the time and was named with a weird project name so I didn’t realise that’s it. I just didn’t try to look for it hard enough.\nHere are a couple of videos I took back in 2015 when it was built. The script was running on a Raspberry Pi 3 with a USB sound card connecting to a microphone and a speaker.\nYou will notice that it was pretty slow in response. That’s probably because I couldn’t figure out how to lower the sound input sample rate. The rate was 44.1KHz. I think 8KHz is good enough and it can cut down the data size significantly.\nHow the script works is fairly simple. Get the voice data from the USB sound card, and send it to Google Speech Recognition API if the sound goes above a threshold. Get the text back from Google API and decide what to do. It can only handle two kinds of commands. If you say “(.)(tell|say)(.) about (.)”, it will call Wikipedia for the last “(.)” which is supposed to be a name or a thing. Otherwise, it will call WolframAlpha to try to get an answer to your question. Once it gets the text for the response, it responds through the speaker.\nThe most challenging part of this project was not coding. The original plan was to use a Bluetooth speaker with microphone and I spent a long time trying to make the speaker work with Raspberry Pi but failed.\nNow, here is the source code. I can’t guarantee it still works since it’s over 2 years old. It might not even be the working version on my Raspberry Pi. But I hope this is helpful for anyone who is interesting in building a voice assistant.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 import speech_recognition as sr import time import pyttsx import threading import wikipedia import re import wolframalpha def listen(): def say(text): engine = pyttsx.init() engine.setProperty(\u0026#34;rate\u0026#34;, 150) engine.setProperty(\u0026#34;volume\u0026#34;, 0.3) engine.say(text) engine.runAndWait() def wiki(text): summary = wikipedia.summary(text, sentences=3) summary = re.sub(\u0026#39;\\([^\\)]*\\)\u0026#39;, \u0026#39;\u0026#39;, summary) summary = re.sub(\u0026#39;\\/[^\\/]*\\/\u0026#39;, \u0026#39;\u0026#39;, summary) return summary def wolfra(question): result = \u0026#34;\u0026#34; client = wolframalpha.Client(\u0026#34;Your WolframAlpha API Key\u0026#34;) res = client.query(speechtext) if(len(res.pods) \u0026gt; 0): results = list(res.results) result = results[0].text return result r = sr.Recognizer() m = sr.Microphone() m.RATE = 44100 m.CHUNK = 512 say(\u0026#34;How can I help you?\u0026#34;) print(\u0026#34;A moment of silence, please...\u0026#34;) with m as source: r.adjust_for_ambient_noise(source) if(r.energy_threshold \u0026lt; 2000): r.energy_threshold = 2000 print(\u0026#34;Set minimum energy threshold to {}\u0026#34;.format(r.energy_threshold)) print(\u0026#34;Say something!\u0026#34;) audio = r.listen(source) print(\u0026#34;Got it! Now to recognize it...\u0026#34;) say(\u0026#34;one moment, let me think\u0026#34;) try: speechtext = r.recognize(audio) print(\u0026#34;You said: \u0026#34; + speechtext) pattern = re.compile(\u0026#34;(.*)(tell|say)(.*) about (.*)\u0026#34;) result = pattern.match(speechtext) if(result != None): answer = wiki(result.group(4)) print(answer) say(answer) else: answer = wolfra(speechtext) if not answer: print(\u0026#34;sorry, I don\u0026#39;t know\u0026#34;) say(\u0026#34;sorry, I don\u0026#39;t know\u0026#34;) else: print(answer) say(answer) except LookupError: print(\u0026#34;sorry, I didn\u0026#39;t catch that\u0026#34;) say(\u0026#34;sorry, I didn\u0026#39;t catch that\u0026#34;) while True: listen() If you want to build a voice assistant, I suggest trying DialogFlow as the response engine. It can make responses more natural, and it can have a conversation to gather all the required information for a task (e.g. book a flight ticket).\n","date":"2018-01-15T00:00:00Z","image":"https://chaoming.li/blog/python-voice-assistant-source-code/pytho-voice-assistant_hu7b241016da3341289301a9a33051330c_899511_120x120_fill_box_smart1_3.png","permalink":"https://chaoming.li/blog/python-voice-assistant-source-code/","title":"Python Voice Assistant Source Code"}]